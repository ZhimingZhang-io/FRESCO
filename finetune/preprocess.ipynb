{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for preprocessing PTBXL, CPSC2018, and CSN datasets for finetuning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import os\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal as scipy_signal\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the split file path to store your processed csv file\n",
    "split_path = './data_split/'\n",
    "# set the meta path for the raw ecg you download\n",
    "meta_path = '../Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing PTB-XL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since PTB-XL provide the offical split, we will use the offical split for the finetune dataset.\n",
    "The offical preprocess code is shown in the orignal paper: https://www.nature.com/articles/s41597-020-0495-6\n",
    "We also list the preprocessed csv file in MERL/finetune/data_split/ptbxl\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CPSC2018 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This dataset provide raw file in .mat format.\n",
    "We first convert the .mat file to .hea and .dat file using the wfdb package.\n",
    "Then we downsample the data to 100Hz and 500Hz.\n",
    "All information of this dataset can be found in: http://2018.icbeb.org/Challenge.html\n",
    "'''\n",
    "\n",
    "# here is your original data folder, you should download the data from the website\n",
    "ori_data_folder = os.path.join(meta_path, 'CPSC2018')\n",
    "\n",
    "# here is the output folder to store the preprocessed data\n",
    "output_folder = os.path.join(meta_path, 'icbeb2018')\n",
    "output_datafolder_100 = output_folder+ '/records100/'\n",
    "output_datafolder_500 = output_folder+ '/records500/'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "else:\n",
    "    print('The folder already exists')\n",
    "if not os.path.exists(output_datafolder_100):\n",
    "    os.makedirs(output_datafolder_100)\n",
    "else:\n",
    "    print('The folder already exists')\n",
    "if not os.path.exists(output_datafolder_500):\n",
    "    os.makedirs(output_datafolder_500)\n",
    "else:\n",
    "    print('The folder already exists')\n",
    "\n",
    "# function to store 12 leads ECG data as wfdb format\n",
    "def store_as_wfdb(signame, data, sigfolder, fs):\n",
    "    channel_itos=['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    wfdb.wrsamp(signame,\n",
    "                fs=fs,\n",
    "                sig_name=channel_itos, \n",
    "                p_signal=data,\n",
    "                units=['mV']*len(channel_itos),\n",
    "                fmt = ['16']*len(channel_itos), \n",
    "                write_dir=sigfolder)  \n",
    "\n",
    "# load the reference csv file\n",
    "reference_path = os.path.join(output_folder, 'REFERENCE.csv')\n",
    "df_reference = pd.read_csv(reference_path)\n",
    "\n",
    "# define the label dictionary\n",
    "# label_dict = {1:'NORM', 2:'AFIB', 3:'1AVB', 4:'CLBBB', 5:'CRBBB', 6:'PAC', 7:'VPC', 8:'STD_', 9:'STE_'}\n",
    "label_dict = {1:'NORM', 2:'AFIB', 3:'1AVB', 4:'CLBBB', 5:'CRBBB', 6:'PAC', 7:'VPC', 8:'STD', 9:'STE'}\n",
    "\n",
    "data = {'ecg_id':[], 'filename':[], 'validation':[], 'age':[], 'sex':[], 'scp_codes':[]}\n",
    "\n",
    "# read all .mat files from the folder then convert to .hea and .dat files\n",
    "ecg_counter = 0\n",
    "# filename = 'A0001.mat'\n",
    "# mat = loadmat(ori_data_folder + '/' +  filename)\n",
    "# print(mat['val'])\n",
    "\n",
    "\n",
    "filenames = os.listdir(ori_data_folder)\n",
    "for filename in tqdm(filenames):\n",
    "    if filename.split('.')[1] == 'mat':\n",
    "        name = filename.split('.')[0]\n",
    "        hea_path = ori_data_folder + '/' +  name + '.hea'\n",
    "        with open(hea_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        age = -1\n",
    "        sex = 'Unknown'\n",
    "        for line in lines:\n",
    "            if '#Age' in line:\n",
    "                age = int(line.split(':')[1].strip()) if line.split(':')[1].strip().isdigit() else -1\n",
    "            if '#Sex' in line:\n",
    "                sex = line.split(':')[1].strip()\n",
    "        if age < 0 or sex == 'Unknown':\n",
    "            continue\n",
    "        ecg_counter += 1\n",
    "        sig = loadmat(ori_data_folder + '/' + filename)['val']\n",
    "        sig = sig.astype(np.float32)\n",
    "        data['ecg_id'].append(ecg_counter)\n",
    "        data['filename'].append(name)\n",
    "        data['validation'].append(False)\n",
    "        data['age'].append(age)\n",
    "        data['sex'].append(1 if sex == 'Male' else 0)\n",
    "        labels = df_reference[df_reference.Recording == name][['First_label' ,'Second_label' ,'Third_label']].values.flatten()\n",
    "        labels = labels[~np.isnan(labels)].astype(int)\n",
    "        data['scp_codes'].append({label_dict[key]:1 for key in labels})\n",
    "\n",
    "        # resample to 500 hz data\n",
    "        store_as_wfdb(str(ecg_counter), sig.T, output_datafolder_500, 500)\n",
    "        # resample to 100 hz data\n",
    "        down_sig = np.array([zoom(channel, .2) for channel in sig])\n",
    "        store_as_wfdb(str(ecg_counter), down_sig.T, output_datafolder_100, 100)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['patient_id'] = df.ecg_id\n",
    "# df = stratisfy_df(df, 'strat_fold')\n",
    "# df.to_csv(output_folder+'icbeb_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the patient_id column the first column\n",
    "cols = list(df.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "switched_df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique labels from the 'scp_codes' column\n",
    "# all_labels = set()\n",
    "# for item in switched_df['scp_codes']:\n",
    "#     all_labels.update(item.keys())\n",
    "\n",
    "all_labels = ['AFIB', 'VPC', 'NORM', '1AVB', 'CRBBB', 'STE', 'PAC', 'CLBBB', 'STD']\n",
    "\n",
    "\n",
    "# # Create new columns for each label\n",
    "for label in all_labels:\n",
    "    switched_df[label] = switched_df['scp_codes'].apply(lambda x: x.get(label, 0))\n",
    "\n",
    "cols = list(switched_df.columns)\n",
    "print(cols)\n",
    "# cols[-1] = 'STD'\n",
    "# cols[-4] = 'STE'\n",
    "# # replace columns name\n",
    "# switched_df.columns = cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test val\n",
    "train_df, test_df = train_test_split(switched_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f'train_df shape: {train_df.shape}')\n",
    "print(f'val_df shape: {val_df.shape}')\n",
    "print(f'test_df shape: {test_df.shape}')\n",
    "\n",
    "# save the csv files\n",
    "train_df.to_csv(split_path+'/icbeb/icbeb_train.csv', index=False)\n",
    "val_df.to_csv(split_path+'/icbeb/icbeb_val.csv', index=False)\n",
    "test_df.to_csv(split_path+'/icbeb/icbeb_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CSN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [06:20<00:00,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理完成:\n",
      "成功记录: 45097\n",
      "失败记录: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 重新整理你的CSN数据处理代码\n",
    "your_path = '../Dataset/'\n",
    "data_path = f'{your_path}CSN/WFDBRecords'\n",
    "\n",
    "# 初始化数据字典\n",
    "df = {'ecg_path': [], 'age': [], 'diagnose': []}\n",
    "\n",
    "# 读取参考文件\n",
    "ref = pd.read_csv(f'{your_path}CSN/ConditionNames_SNOMED-CT.csv')\n",
    "ref['Snomed_CT'] = ref['Snomed_CT'].astype(str)\n",
    "\n",
    "def read_header_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        header_info = [line.strip() for line in lines]\n",
    "    return header_info\n",
    "\n",
    "def extract_age_from_hea(hea_lines):\n",
    "    \"\"\"从.hea文件提取年龄\"\"\"\n",
    "    for line in hea_lines:\n",
    "        if '#Age' in line:\n",
    "            try:\n",
    "                age_str = line.split(':')[1].strip()\n",
    "                if age_str.lower() not in ['nan', 'unknown', '', 'null']:\n",
    "                    return int(float(age_str))\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    return -1\n",
    "\n",
    "def extract_diagnose_from_hea(hea_lines, ref_df):\n",
    "    \"\"\"从.hea文件提取诊断信息\"\"\"\n",
    "    diagnoses = []\n",
    "    for line in hea_lines:\n",
    "        if '#Dx' in line:\n",
    "            try:\n",
    "                dx_codes = line.split(':')[1].strip()\n",
    "                if dx_codes and dx_codes != 'Unknown':\n",
    "                    codes = dx_codes.split(',')\n",
    "                    for code in codes:\n",
    "                        code = code.strip()\n",
    "                        matched = ref_df[ref_df['Snomed_CT'] == code]\n",
    "                        if not matched.empty:\n",
    "                            diagnoses.append(matched['Acronym Name'].iloc[0])\n",
    "            except IndexError:\n",
    "                continue\n",
    "    return ','.join(diagnoses) if diagnoses else 'Unknown'\n",
    "\n",
    "# 处理数据文件夹\n",
    "folders = os.listdir(data_path)\n",
    "folders = sorted([os.path.join(data_path, f) for f in folders if os.path.isdir(os.path.join(data_path, f))])\n",
    "\n",
    "successful_records = 0\n",
    "failed_records = 0\n",
    "\n",
    "for i, folder in enumerate(tqdm(folders)):\n",
    "    subfolders = os.listdir(folder)\n",
    "    subfolders = sorted([os.path.join(folder, f) for f in subfolders if os.path.isdir(os.path.join(folder, f))])\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        try:\n",
    "            files = os.listdir(subfolder)\n",
    "            mat_files = sorted([f for f in files if f.endswith('.mat')])\n",
    "            hea_files = sorted([f for f in files if f.endswith('.hea')])\n",
    "            \n",
    "            # 确保mat和hea文件一一对应\n",
    "            for mat_file in mat_files:\n",
    "                base_name = mat_file.replace('.mat', '')\n",
    "                hea_file = base_name + '.hea'\n",
    "                \n",
    "                if hea_file in hea_files:\n",
    "                    mat_path = os.path.join(subfolder, mat_file)\n",
    "                    hea_path = os.path.join(subfolder, hea_file)\n",
    "                    \n",
    "                    try:\n",
    "                        # 读取ECG数据\n",
    "                        mat = loadmat(mat_path)\n",
    "                        ecg = mat['val']\n",
    "                        \n",
    "                        # 读取头文件\n",
    "                        hea_lines = read_header_file(hea_path)\n",
    "                        \n",
    "                        # 提取年龄\n",
    "                        age = extract_age_from_hea(hea_lines)\n",
    "                        if age < 0 or age > 120:  # 年龄合理性检查\n",
    "                            failed_records += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # 提取诊断\n",
    "                        diagnose = extract_diagnose_from_hea(hea_lines, ref)\n",
    "                        \n",
    "                        # 添加到数据字典 - 确保同时添加所有字段\n",
    "                        relative_path = os.path.relpath(mat_path, start=your_path)\n",
    "                        df['ecg_path'].append(relative_path)\n",
    "                        df['age'].append(age)\n",
    "                        df['diagnose'].append(diagnose)\n",
    "                        \n",
    "                        successful_records += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"处理文件 {mat_path} 时出错: {e}\")\n",
    "                        failed_records += 1\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"处理文件夹 {subfolder} 时出错: {e}\")\n",
    "            failed_records += 1\n",
    "            continue\n",
    "\n",
    "print(f\"\\n处理完成:\")\n",
    "print(f\"成功记录: {successful_records}\")\n",
    "print(f\"失败记录: {failed_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据验证:\n",
      "ecg_path: 45097 条记录\n",
      "age: 45097 条记录\n",
      "diagnose: 45097 条记录\n",
      "有效诊断记录: 44422\n",
      "发现 51 种诊断类别\n",
      "最终DataFrame形状: (44422, 54)\n",
      "标签分布:\n",
      "  SA: 3552\n",
      "  UW: 136\n",
      "  RVH: 109\n",
      "  STE: 800\n",
      "  CCR: 162\n",
      "  MI: 120\n",
      "  ALS: 1543\n",
      "  2AVB1: 31\n",
      "  VB: 1551\n",
      "  VFW: 115\n",
      "  JPT: 11\n",
      "  ARS: 847\n",
      "  WAVN: 2\n",
      "  TWC: 7028\n",
      "  PRIE: 52\n",
      "  SR: 8123\n",
      "  LBBB: 240\n",
      "  AVB: 1548\n",
      "  3AVB: 76\n",
      "  LVH: 642\n",
      "  TWO: 2875\n",
      "  AVRT: 26\n",
      "  VET: 8\n",
      "  1AVB: 1140\n",
      "  SVT: 700\n",
      "  ST: 9858\n",
      "  ERV: 366\n",
      "  AQW: 1062\n",
      "  APB: 1312\n",
      "  QTIE: 391\n",
      "  CR: 238\n",
      "  AT: 297\n",
      "  VEB: 56\n",
      "  JEB: 75\n",
      "  RAH: 36\n",
      "  VPE: 12\n",
      "  IDC: 767\n",
      "  FQRS: 3\n",
      "  2AVB: 97\n",
      "  RBBB: 649\n",
      "  LVQRSAL: 1039\n",
      "  ABI: 3\n",
      "  WPW: 72\n",
      "  AF: 9809\n",
      "  STDD: 1668\n",
      "  STTU: 176\n",
      "  AFIB: 1780\n",
      "  SB: 16559\n",
      "  PWC: 142\n",
      "  VPB: 294\n",
      "  STTC: 1158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 验证数据一致性后创建DataFrame\n",
    "print(f\"\\n数据验证:\")\n",
    "for key, value in df.items():\n",
    "    print(f\"{key}: {len(value)} 条记录\")\n",
    "\n",
    "# 确保所有列长度一致\n",
    "if len(set(len(v) for v in df.values())) == 1:\n",
    "    new_df = pd.DataFrame(df)\n",
    "    \n",
    "    # 过滤掉诊断为Unknown的记录\n",
    "    new_df = new_df[new_df['diagnose'] != 'Unknown']\n",
    "    new_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print(f\"有效诊断记录: {len(new_df)}\")\n",
    "    \n",
    "    # 创建多标签列\n",
    "    if len(new_df) > 0:\n",
    "        unique_labels = []\n",
    "        for labels in new_df['diagnose']:\n",
    "            if labels and labels != 'Unknown':\n",
    "                labels = labels.split(',')\n",
    "                unique_labels.extend([label.strip() for label in labels])\n",
    "        \n",
    "        unique_labels = list(set(unique_labels))\n",
    "        print(f\"发现 {len(unique_labels)} 种诊断类别\")\n",
    "        \n",
    "        # 为每个标签创建二进制列\n",
    "        for label in unique_labels:\n",
    "            new_df[label] = new_df['diagnose'].apply(lambda x: 1 if label in x else 0)\n",
    "        \n",
    "        print(f\"最终DataFrame形状: {new_df.shape}\")\n",
    "        print(\"标签分布:\")\n",
    "        for label in unique_labels:\n",
    "            count = new_df[label].sum()\n",
    "            print(f\"  {label}: {count}\")\n",
    "            \n",
    "else:\n",
    "    print(\"错误: 数据字典中各列长度不一致，无法创建DataFrame\")\n",
    "    for key, value in df.items():\n",
    "        print(f\"  {key}: {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of sample for each label\n",
    "label_count = {}\n",
    "for label in unique_labels:\n",
    "    label_count[label] = new_df[label].sum()\n",
    "# sort the label_count dictionary\n",
    "label_count = dict(sorted(label_count.items(), key=lambda item: item[1], reverse=True))\n",
    "# drop the label with less than 10 samples\n",
    "for key in list(label_count.keys()):\n",
    "    if label_count[key] < 10:\n",
    "        del label_count[key]\n",
    "# drop the columns not in label_count\n",
    "essential_cols = ['ecg_path', 'age', 'diagnose']\n",
    "for key in list(new_df.columns):\n",
    "    if key not in label_count.keys() and key not in essential_cols:\n",
    "        new_df.drop(key, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (31983, 50)\n",
      "val_df shape: (3554, 50)\n",
      "test_df shape: (8885, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split train test val\n",
    "\n",
    "train_df, test_df = train_test_split(new_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f'train_df shape: {train_df.shape}')\n",
    "print(f'val_df shape: {val_df.shape}')\n",
    "print(f'test_df shape: {test_df.shape}')\n",
    "\n",
    "# save the csv files\n",
    "train_df.to_csv(f'{split_path}chapman/'+'chapman_train.csv', index=False)\n",
    "val_df.to_csv(f'{split_path}chapman/'+'chapman_val.csv', index=False)\n",
    "test_df.to_csv(f'{split_path}chapman/'+'chapman_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
